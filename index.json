[{"authors":null,"categories":null,"content":"","date":1461124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461124800,"objectID":"dca99e65ea8f5db32c308fd234afbfcd","permalink":"https://chengxuz.github.io/publications_selected/","publishdate":"2016-04-20T00:00:00-04:00","relpermalink":"/publications_selected/","section":"","summary":"","tags":null,"title":"Selected Publications","type":"page"},{"authors":null,"categories":null,"content":"","date":1461124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461124800,"objectID":"efb6458f3acff3dbd748b90dd629c79c","permalink":"https://chengxuz.github.io/talks_selected/","publishdate":"2016-04-20T00:00:00-04:00","relpermalink":"/talks_selected/","section":"","summary":"","tags":null,"title":"Selected Talks","type":"page"},{"authors":null,"categories":null,"content":"","date":1461124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461124800,"objectID":"49cbbfcf1124f111a37fcbac0faeb48e","permalink":"https://chengxuz.github.io/talks/","publishdate":"2016-04-20T00:00:00-04:00","relpermalink":"/talks/","section":"","summary":"","tags":null,"title":"Recent \u0026 Upcoming Talks","type":"page"},{"authors":null,"categories":null,"content":"","date":1461124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461124800,"objectID":"8a648aa279e347a7f7d13144c3d1dff5","permalink":"https://chengxuz.github.io/posts/","publishdate":"2016-04-20T00:00:00-04:00","relpermalink":"/posts/","section":"","summary":"","tags":null,"title":"Recent Posts","type":"page"},{"authors":null,"categories":null,"content":"","date":1461124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461124800,"objectID":"3c42bb4157bf51d85ae7ffc4e1685909","permalink":"https://chengxuz.github.io/projects/","publishdate":"2016-04-20T00:00:00-04:00","relpermalink":"/projects/","section":"","summary":"","tags":null,"title":"Projects","type":"page"},{"authors":null,"categories":null,"content":"","date":1505880000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505880000,"objectID":"97f061f6e721d99ff473e04294bd8fcf","permalink":"https://chengxuz.github.io/tags/","publishdate":"2017-09-20T00:00:00-04:00","relpermalink":"/tags/","section":"","summary":"","tags":null,"title":"Tags","type":"page"},{"authors":null,"categories":null,"content":"","date":1532318400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532318400,"objectID":"4953bf99d7018c161b1b361cb25f1331","permalink":"https://chengxuz.github.io/search/","publishdate":"2018-07-23T00:00:00-04:00","relpermalink":"/search/","section":"","summary":"","tags":null,"title":"Search","type":"page"},{"authors":["Bria Long","Sarah Goodin","George Kachergis","Virginia A Marchman","Samaher F Radwan","Robert Z Sparks","Violet Xiang","Chengxu Zhuang","Oliver Hsu","Brett Newman","Daniel LK Yamins","Michael C Frank"],"categories":null,"content":"","date":1698293826,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698293826,"objectID":"3cb6845f6291347934133499db25e364","permalink":"https://chengxuz.github.io/publication/babyview/","publishdate":"2023-10-25T21:17:06-07:00","relpermalink":"/publication/babyview/","section":"publication","summary":"Head-mounted cameras have been used in developmental psychology research for more than a decade to provide a rich and comprehensive view of what infants see during their everyday experiences. However, variation between these devices has limited the field’s ability to compare results across studies and across labs. Further, the video data captured by these cameras to date has been relatively low-resolution, limiting how well machine learning algorithms can operate over these rich video data. Here, we provide a well-tested and easily constructed design for a head-mounted camera assembly—the BabyView—developed in collaboration with Daylight Design, LLC., a professional product design firm. The BabyView collects high-resolution video, accelerometer, and gyroscope data from children approximately 6 - 30 months of age via a GoPro camera custom mounted on a soft child-safety helmet. The BabyView also captures a large, portrait-oriented vertical field-of-view that encompasses both children’s interactions with objects and with their social partners. We detail our protocols for video data management and for handling sensitive data from home environments. We also provide customizable materials for onboarding families with the BabyView. We hope that these materials will encourage the wide adoption of the BabyView, allowing the field to collect high-resolution data that can link children’s everyday environments with their learning outcomes.","tags":[],"title":"The BabyView camera: Designing a new head-mounted camera to capture children’s early social and visual environments","type":"publication"},{"authors":["Chengxu Zhuang","Evelina Fedorenko","Jacob Andreas"],"categories":null,"content":"","date":1698293826,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698293826,"objectID":"f0003d0b7827c6170e7821584a2741bf","permalink":"https://chengxuz.github.io/publication/visual_grounding/","publishdate":"2023-10-25T21:17:06-07:00","relpermalink":"/publication/visual_grounding/","section":"publication","summary":"Modern neural language models (LMs) are powerful tools for modeling human sentence production and comprehension, and their internal representations are remarkably well-aligned with representations of language in the human brain. But to achieve these results, LMs must be trained in distinctly un-human-like ways—requiring orders of magnitude more language data than children receive during development, and without any of the accompanying grounding in perception, action, or social behavior. Do models trained more naturalistically—with grounded supervision—exhibit more human-like language learning? We investigate this question in the context of word learning, a key sub-task in language acquisition. We train a diverse set of LM architectures, with and without auxiliary supervision from image captioning tasks, on datasets of varying scales. We then evaluate these models on a broad set of benchmarks characterizing models’ learning of syntactic categories, lexical relations, semantic features, semantic similarity, and alignment with human neural representations. We find that visual supervision can indeed improve the efficiency of word learning. However, these improvements are limited: they are present almost exclusively in the low-data regime, and sometimes canceled out by the inclusion of rich distributional signals from text. The information conveyed by text and images is not redundant—we find that models mainly driven by visual information yield qualitatively different from those mainly driven by word co-occurrences. However, our results suggest that current multi-modal modeling approaches fail to effectively leverage visual information to build more human-like word representations from human-sized datasets.","tags":[],"title":"Visual Grounding Helps Learn Word Meanings in Low-Data Regimes","type":"publication"},{"authors":["**Chengxu Zhuang**","**Violet Xiang**","Yoon Bai","Xiaoxuan Jia","Nicholas Turk-Browne","Kenneth Norman","James DiCarlo","Daniel Yamins"],"categories":null,"content":"","date":1663388226,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663388226,"objectID":"a6bd5c7310a0999ba16ca22cd89a7e53","permalink":"https://chengxuz.github.io/publication/short_long_bms/","publishdate":"2022-09-16T21:17:06-07:00","relpermalink":"/publication/short_long_bms/","section":"publication","summary":"Humans learn from visual inputs at multiple timescales, both rapidly and flexibly acquiring visual knowledge over short periods, and robustly accumulating online learning progress over longer periods. Modeling these powerful learning capabilities is an important problem for computational visual cognitive science, and models that could replicate them would be of substantial utility in real-world computer vision settings. In this work, we establish benchmarks for both real-time and life-long continual visual learning. Our real-time learning benchmark measures a model's ability to match the rapid visual behavior changes of real humans over the course of minutes and hours, given a stream of visual inputs. Our life-long learning benchmark evaluates the performance of models in a purely online learning curriculum obtained directly from child visual experience over the course of years of development. We evaluate a spectrum of recent deep self-supervised visual learning algorithms on both benchmarks, finding that none of them perfectly match human performance, though some algorithms perform substantially better than others. Interestingly, algorithms embodying recent trends in self-supervised learning -- including BYOL, SwAV and MAE -- are substantially worse on our benchmarks than an earlier generation of self-supervised algorithms such as SimCLR and MoCo-v2. We present analysis indicating that the failure of these newer algorithms is primarily due to their inability to handle the kind of sparse low-diversity datastreams that naturally arise in the real world, and that actively leveraging memory through negative sampling -- a mechanism eschewed by these newer algorithms -- appears useful for facilitating learning in such low-diversity environments. We also illustrate a complementarity between the short and long timescales in the two benchmarks, showing how requiring a single learning algorithm to be locally context-sensitive enough to match real-time learning changes while stable enough to avoid catastrophic forgetting over the long term induces a trade-off that human-like algorithms may have to straddle. Taken together, our benchmarks establish a quantitative way to directly compare learning between neural networks models and human learners, show how choices in the mechanism by which such algorithms handle sample comparison and memory strongly impact their ability to match human learning abilities, and expose an open problem space for identifying more flexible and robust visual self-supervision algorithms.","tags":[],"title":"How Well Do Unsupervised Learning Algorithms Model Human Real-time and Life-long Learning?","type":"publication"},{"authors":["Mike Wu","Milan Mosse","Chengxu Zhuang","Daniel Yamins","Noah Goodman"],"categories":null,"content":"","date":1616213826,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616213826,"objectID":"9233b5a628b950b19434f40b3870e2fc","permalink":"https://chengxuz.github.io/publication/contrast_info/","publishdate":"2021-03-19T21:17:06-07:00","relpermalink":"/publication/contrast_info/","section":"publication","summary":"Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two views of an image. NCE uses randomly sampled negative examples to normalize the objective. In this paper, we show that choosing difficult negatives, or those more similar to the current instance, can yield stronger representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a 'ring' around each positive. We prove that these estimators lower-bound mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% points in each case, measured by linear evaluation on four standard image datasets. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and keypoint detection.","tags":[],"title":"Conditional Negative Sampling for Contrastive Learning of Visual Representations.","type":"publication"},{"authors":["Chengxu Zhuang","Siming Yan","Aran Nayebi","Martin Schrimpf","Michael C. Frank","James J. DiCarlo","Daniel Yamins"],"categories":null,"content":"","date":1611116226,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611116226,"objectID":"bc1bae08dc4bedad13d01fb35ae1861b","permalink":"https://chengxuz.github.io/publication/unsup_vvs/","publishdate":"2021-01-19T21:17:06-07:00","relpermalink":"/publication/unsup_vvs/","section":"publication","summary":"Deep neural networks currently provide the best quantitative models of the response patterns of neurons throughout the primate ventral visual stream. However, such networks have remained implausible as a model of the development of the ventral stream, in part because they are trained with supervised methods requiring many more labels than are accessible to infants during development. Here, we report that recent rapid progress in unsupervised learning has largely closed this gap. We find that neural network models learned with deep unsupervised contrastive embedding methods achieve neural prediction accuracy in multiple ventral visual cortical areas that equals or exceeds that of models derived using today’s best supervised methods and that the mapping of these neural network models’ hidden layers is neuroanatomically consistent across the ventral stream. Strikingly, we find that these methods produce brain-like representations even when trained solely with real human child developmental data collected from head-mounted cameras, despite the fact that these datasets are noisy and limited. We also find that semisupervised deep contrastive embeddings can leverage small numbers of labeled examples to produce representations with substantially improved error-pattern consistency to human behavior. Taken together, these results illustrate a use of unsupervised learning to provide a quantitative model of a multiarea cortical brain system and present a strong candidate for a biologically plausible computational theory of primate sensory learning.","tags":[],"title":"Unsupervised neural network models of the ventral visual stream","type":"publication"},{"authors":["Chengxu Zhuang","Tianwei She","Alex Andonian","Max Sobol Mark","Daniel Yamins"],"categories":null,"content":"","date":1580530626,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580530626,"objectID":"8b91517d6f3a1e1b86203073c31d1cf5","permalink":"https://chengxuz.github.io/publication/vie/","publishdate":"2020-01-31T21:17:06-07:00","relpermalink":"/publication/vie/","section":"publication","summary":"Because of the rich dynamical structure of videos and their ubiquity in everyday life, it is a natural idea that video data could serve as a powerful unsupervised learning signal for visual representations. However, instantiating this idea, especially at large scale, has remained a significant artificial intelligence challenge. Here we present the Video Instance Embedding (VIE) framework, which trains deep nonlinear embeddings on video sequence inputs. By learning embedding dimensions that identify and group similar videos together, while pushing inherently different videos apart in the embedding space, VIE captures the strong statistical structure inherent in videos, without the need for external annotation labels. We find that, when trained on a large-scale video dataset, VIE yields powerful representations both for action recognition and single-frame object categorization, showing substantially improving on the state of the art wherever direct comparisons are possible. We show that a twopathway model with both static and dynamic processing pathways is optimal, provide analyses indicating how the model works, and perform ablation studies showing the importance of key architecture and loss function choices. Our results suggest that deep neural embeddings are a promising approach to unsupervised video learning for a wide variety of task domains.","tags":[],"title":"Unsupervised Learning from Video with Deep Neural Embeddings","type":"publication"},{"authors":["Chengxu Zhuang","Alex Lin Zhai","Daniel Yamins"],"categories":null,"content":"","date":1548994626,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548994626,"objectID":"c576aea92e73db5a481caf5bcc61e572","permalink":"https://chengxuz.github.io/publication/local_aggregation/","publishdate":"2019-01-31T21:17:06-07:00","relpermalink":"/publication/local_aggregation/","section":"publication","summary":"Unsupervised approaches to learning in neural networks are of substantial interest for furthering artificial intelligence, both because they would enable the training of networks without the need for large numbers of expensive annotations, and because they would be better models of the kind of general-purpose learning deployed by humans. However, unsupervised networks have long lagged behind the performance of their supervised counterparts, especially in the domain of large-scale visual recognition. Recent developments in training deep convolutional embeddings to maximize non-parametric instance separation and clustering objectives have shown promise in closing this gap. Here, we describe a method that trains an embedding function to maximize a metric of local aggregation, causing similar data instances to move together in the embedding space, while allowing dissimilar instances to separate. This aggregation metric is dynamic, allowing soft clusters of different scales to emerge. We evaluate our procedure on several large-scale visual recognition datasets, achieving state-of-the-art unsupervised transfer learning performance on object recognition in ImageNet, scene recognition in Places 205, and object detection in PASCAL VOC.","tags":[],"title":"Local Aggregation for Unsupervised Learning of Visual Embeddings","type":"publication"},{"authors":["**Damian Mrowca**","**Chengxu Zhuang**","**Elias Wang**","Nick Haber","Li Fei-Fei","Joshua B. Tenenbaum","Daniel Yamins"],"categories":null,"content":"","date":1526012226,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526012226,"objectID":"25b90aec8451bd4b7d96ce25ac9e22a6","permalink":"https://chengxuz.github.io/publication/physics/","publishdate":"2018-05-10T21:17:06-07:00","relpermalink":"/publication/physics/","section":"publication","summary":"Humans have a remarkable capacity to understand the physical dynamics of objects in their environment, flexibly capturing complex structures and interactions at multiple levels of detail. Inspired by this ability, we propose a hierarchical particlebased object representation that covers a wide variety of types of three-dimensional objects, including both arbitrary rigid geometrical shapes and deformable materials. We then describe the Hierarchical Relation Network (HRN), an end-to-end differentiable neural network based on hierarchical graph convolution, that learns to predict physical dynamics in this representation. Compared to other neural network baselines, the HRN accurately handles complex collisions and nonrigid deformations, generating plausible dynamics predictions at long time scales in novel settings, and scaling to large scene configurations. These results demonstrate an architecture with the potential to form the basis of next-generation physics predictors for use in computer vision, robotics, and quantitative cognitive science.","tags":[],"title":"Flexible Neural Representation for Physics Prediction","type":"publication"},{"authors":["Chengxu Zhuang","Jonas Kubilius","Mitra Hartmann","Daniel Yamins"],"categories":null,"content":"","date":1512964281,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512964281,"objectID":"e624bbc294c504db32e12336c994ee48","permalink":"https://chengxuz.github.io/publication/whisker/","publishdate":"2017-12-10T20:51:21-07:00","relpermalink":"/publication/whisker/","section":"publication","summary":"In large part, rodents “see” the world through their whiskers, a powerful tactile sense enabled by a series of brain areas that form the whisker-trigeminal system. Raw sensory data arrives in the form of mechanical input to the exquisitely sensitive, actively-controllable whisker array, and is processed through a sequence of neural circuits, eventually arriving in cortical regions that communicate with decision- making and memory areas. Although a long history of experimental studies has characterized many aspects of these processing stages, the computational operations of the whisker-trigeminal system remain largely unknown. In the present work, we take a goal-driven deep neural network (DNN) approach to modeling these computations. First, we construct a biophysically-realistic model of the rat whisker array. We then generate a large dataset of whisker sweeps across a wide variety of 3D objects in highly-varying poses, angles, and speeds. Next, we train DNNs from several distinct architectural families to solve a shape recognition task in this dataset. Each architectural family represents a structurally-distinct hypothesis for processing in the whisker-trigeminal system, corresponding to different ways in which spatial and temporal information can be integrated. We find that most networks perform poorly on the challenging shape recognition task, but that specific architectures from several families can achieve reasonable performance levels. Finally, we show that Representational Dissimilarity Matrices (RDMs), a tool for comparing population codes between neural systems, can separate these higher-performing networks with data of a type that could plausibly be collected in a neurophysiological or imaging experiment. Our results are a proof-of-concept that DNN models of the whisker-trigeminal system are potentially within reach.","tags":[],"title":"Toward Goal-Driven Neural Network Models for the Rodent Whisker-Trigeminal System","type":"publication"},{"authors":["Chengxu Zhuang","Yulong Wang","Daniel Yamins","Xiaolin Hu"],"categories":null,"content":"","date":1508302309,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508302309,"objectID":"2c7a6caf13a57648de62488b5d744d94","permalink":"https://chengxuz.github.io/publication/signature/","publishdate":"2017-10-17T21:51:49-07:00","relpermalink":"/publication/signature/","section":"publication","summary":"Visual information in the visual cortex is processed in a hierarchical manner. Recent studies show that higher visual areas, such as V2, V3, and V4, respondmore vigorously to images with naturalistic higher-order statistics than to images lacking them. This property is a functional signature of higher areas, as it ismuch weaker or even absent in the primary visual cortex (V1). However, the mechanism underlying this signature remains elusive. We studied this problem using computational models. In several typical hierarchical visual models including the AlexNet, VggNet, and SHMAX, this signature was found to be prominent in higher layers but much weaker in lower layers. By changing both the model structure and experimental settings, we found that the signature strongly correlated with sparse firing of units in higher layers but not with any other factors, including model structure, training algorithm (supervised or unsupervised), receptive field size, and property of training stimuli. The results suggest an important role of sparse neuronal activity underlying this special feature of higher visual areas.","tags":[],"title":"Deep Learning Predicts Correlation between a Functional Signature of Higher Visual Areas and Sparse Firing of Neurons","type":"publication"}]